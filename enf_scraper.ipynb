{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1150e9b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Yeni klas√∂r\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import xlsxwriter\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from string import ascii_uppercase\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff098c5",
   "metadata": {},
   "source": [
    "This project has been developed for web scraping from https://www.enfsolar.com/ website. In order to use the programme, the chrome driver must be compatible with the related version of selenium package. The enf website gives a certain number of access permissions for some information. A dynamically changing ip address vpn can be used to pull data regardless of access permission. Here, only the code information is given, and the relevant VPN is left to the user's preference. When this code runs, information will be accessible to the allowed number of times without VPN. The code asks the user to enter the country number to scrape the company and product information of the relevant country. The first time the code is run, it creates an excel file that holds the scraped information. It creates a worksheet on excel file and its name is country name and saves data on this worksheet. When the code run again it creates a new worksheet which is name new country to scrape and save data in this worksheet. If the code run for same country the code updated data on the worksheet. An example excel file is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f06696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scraping_enfsolar_seller_pages:\n",
    "    \n",
    "    def chrome(headless=False):\n",
    "    # support to get response status and headers\n",
    "        d = webdriver.DesiredCapabilities.CHROME\n",
    "        d['loggingPrefs'] = {'performance': 'ALL'}\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if headless:\n",
    "            options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\")\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument(\"--disable-setuid-sandbox\")\n",
    "            options.add_argument(\"--disable-setuid-sandbox\")\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        chrome_deriver_manager = ChromeDriverManager()\n",
    "        driver = webdriver.Chrome(chrome_deriver_manager.install(), options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        return driver\n",
    "    \n",
    "    def open_enfsolar_seller_page_to_scrape_country(self):\n",
    "        driver.get('https://www.enfsolar.com/directory/seller')\n",
    "        driver.implicitly_wait(3)\n",
    "        parsedHTML = bs(driver.page_source, 'lxml')\n",
    "        urls = []\n",
    "        country = []\n",
    "        counter = 0\n",
    "        for h in parsedHTML.findAll('li'):\n",
    "            a = h.find('a')\n",
    "            try:\n",
    "                if 'href' in a.attrs:\n",
    "                    url = a.get('href')\n",
    "                    pattern = r'^/directory/seller/[A-Za-z_%20]+'\n",
    "                    match = (re.search(pattern, url))\n",
    "                    urls.append(match.group(0))\n",
    "                    pattern2 = r'[^a-zA-Z]+'\n",
    "                    country_name = urls[counter].rsplit('/',1)[-1]\n",
    "                    country_name = re.sub(pattern2, ' ', country_name)\n",
    "                    country.append(country_name)\n",
    "                    counter = counter + 1    \n",
    "            except:\n",
    "                pass\n",
    "        return country, urls\n",
    "\n",
    "    def get_page_results_info(self,parsed): \n",
    "        properties = parsed.find('table', attrs = {'class' : 'enf-list-table'}).tbody.select('td:nth-of-type(1)')\n",
    "        properties2 = parsed.find('table', attrs = {'class' : 'enf-list-table'}).tbody.findAll('td', attrs = {'class' : 'no-left-right-padding'})\n",
    "        c4 = parsed.find('table', attrs = {'class' : 'enf-list-table'}).tbody.findAll('td', attrs = {'class' : 'text-center'})\n",
    "        company_name = []\n",
    "        areas = []\n",
    "        for eachproperty in range(len(properties)):\n",
    "            company_name.append(properties[eachproperty].get_text().strip())\n",
    "        for eachproperty in range(len(properties2)):\n",
    "            areas.append(properties2[eachproperty].get_text().strip())\n",
    "        liste = []\n",
    "        for j in c4:\n",
    "            teams = [img['alt'] for img in j('img')]\n",
    "            if teams == []:\n",
    "                teams = j.get_text().strip()\n",
    "            j.string = ', '.join(teams)\n",
    "            liste.append(teams)\n",
    "            ylist = [liste[i:i + 4] for i in range(0, len(liste), 4)]\n",
    "        return ylist, company_name, areas\n",
    "\n",
    "    def dataframes(self,df4,reference_product_list,business_and_product_names,business_and_product_details):\n",
    "        list_idx1 = [i for i, x in enumerate(reference_product_list) if any([j in x for j in business_and_product_names])]\n",
    "        list_idx2 = [i for i, x in enumerate(business_and_product_names) if any([j in x for j in reference_product_list])] \n",
    "        final_list = ['This product no sales now']*len(reference_product_list)\n",
    "        for i in range(len(list_idx1)):\n",
    "            final_list[list_idx1[i]] = business_and_product_details[list_idx2[i]]\n",
    "        service_coverage_language_spoken_final_list_data.append(final_list[:2])\n",
    "        selector = [0,1]*len(reference_product_list[2:])\n",
    "        new_final_list = final_list[2:]\n",
    "        new_reference_product_list = reference_product_list[2:]\n",
    "        product_details_final_list_data = [new_final_list.pop(0) if x else new_reference_product_list.pop(0) for x in selector]\n",
    "        product_title_and_details_final_list_data.append(product_details_final_list_data)\n",
    "          \n",
    "        info1 = {'Address': company_profile_info_address,\n",
    "                  'Phone' : company_profile_info_telephone,\n",
    "                  'Email': company_profile_info_email,\n",
    "                  'Website': company_profile_info_url,\n",
    "                  'Country': company_profile_info_country,\n",
    "                        }\n",
    "        info1_data.append(info1)\n",
    "        df1 = pd.DataFrame.from_records(info1_data)     \n",
    "        column_names=['Service Coverage','Languages Spoken']\n",
    "        df2 = pd.DataFrame(service_coverage_language_spoken_final_list_data, columns=column_names)\n",
    "        n_repeated_titles = ['Product Title', 'Product Details']*(len(reference_product_list)-2)\n",
    "        df3 = pd.DataFrame(product_title_and_details_final_list_data)\n",
    "        df3.columns = n_repeated_titles       \n",
    "        df5 = pd.concat([df1, df2, df3], axis=1, join='inner')  \n",
    "        df6 = pd.DataFrame(ylist_f, columns = ['Distributor/Wholesaler','Brands Carried','Minimum Order Volume','Products'])\n",
    "        df7 = pd.concat([df4, df6, df5], axis=1, join='inner')     \n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        return df7 \n",
    "    \n",
    "    def replace(self,business_and_product_details):  # Replacing the \\n characters in the item variable with a space character\n",
    "        for i, item in enumerate(business_and_product_details):\n",
    "            item = re.sub(r\"\\s+\", \" \", item)\n",
    "            business_and_product_details[i] = item \n",
    "        return business_and_product_details\n",
    "    \n",
    "    def save_csv(self,df7,country,Country_no):       \n",
    "        df7['Products'] = df7['Products'].astype(str)\n",
    "        df7['Products'] = df7['Products'].astype(str)\n",
    "        df7['Distributor/Wholesaler'] = df7['Distributor/Wholesaler'].astype(str)\n",
    "        df7['Distributor/Wholesaler'] = df7['Distributor/Wholesaler'].astype(str)  \n",
    "        df7['Products'] = df7['Products'].str.replace(r\"\\[\",\"\")\n",
    "        df7['Products'] = df7['Products'].str.replace(r\"\\]\",\"\")\n",
    "        df7['Products'] = df7['Products'].str.replace(r'\\'', '')\n",
    "        df7['Distributor/Wholesaler'] = df7['Distributor/Wholesaler'].str.replace(r\"\\[\",\"\")\n",
    "        df7['Distributor/Wholesaler'] = df7['Distributor/Wholesaler'].str.replace(r\"\\]\",\"\")\n",
    "        df7['Distributor/Wholesaler'] = df7['Distributor/Wholesaler'].str.replace(r'\\'', '')\n",
    "        path = r\"C:\\Users\\ercan\\project_4_web_scraping_2\\enf_seller_results.xlsx\"\n",
    "        if os.path.exists(path) == False:\n",
    "            writer = pd.ExcelWriter(\"enf_seller_results.xlsx\")\n",
    "            df7.to_excel(writer, sheet_name=country[Country_no-1], index = False)\n",
    "            print(\"The excel file was not exist! It was created!\")\n",
    "        else:\n",
    "            wb = openpyxl.load_workbook('enf_seller_results.xlsx')\n",
    "            writer = pd.ExcelWriter('enf_seller_results.xlsx', engine='openpyxl') \n",
    "            writer.book = wb\n",
    "            df7.to_excel(writer, sheet_name=country[Country_no-1], index = False)\n",
    "            print (\"The excel file was exist! Added new sheet or updated!\")   \n",
    "        writer.save()\n",
    "        writer.close()\n",
    "        wb = load_workbook('enf_seller_results.xlsx')\n",
    "        ws = wb[country[Country_no-1]]    \n",
    "        for col in ws.columns:\n",
    "            max_length = 0\n",
    "            column = col[0].column_letter # Get the column name\n",
    "            for cell in col:\n",
    "                try: # Necessary to avoid error on empty cells\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = max_length+3\n",
    "            if adjusted_width > 100:\n",
    "                adjusted_width = 80    \n",
    "            ws.column_dimensions[column].width = adjusted_width          \n",
    "        # Creating red font and white background color for column headings\n",
    "        red_font = Font(color='FF0000', name='Times New Roman', bold=True)\n",
    "        white_fill = PatternFill(start_color='FFFFFF', end_color='FFFFFF', fill_type='solid')\n",
    "        # Coloring all column headings in the worksheet\n",
    "        for column_index in range(1, ws.max_column+1):\n",
    "            column_letter = get_column_letter(column_index)\n",
    "            cell = ws[f'{column_letter}1']\n",
    "            cell.font = red_font\n",
    "            cell.fill = white_fill\n",
    "        for row in ws.rows:\n",
    "            for cell in row:\n",
    "        #Adjusting center alignment for the cell's text\n",
    "                cell.alignment = openpyxl.styles.Alignment(horizontal='center')\n",
    "        wb.save('enf_seller_results.xlsx')\n",
    "        wb.close()\n",
    "        return df7        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Austria\n",
      "2 Belgium\n",
      "3 Czech Republic\n",
      "4 France\n",
      "5 Germany\n",
      "6 Greece\n",
      "7 Hungary\n",
      "8 Italy\n",
      "9 Netherlands\n",
      "10 Poland\n",
      "11 Russia\n",
      "12 Spain\n",
      "13 Switzerland\n",
      "14 United Kingdom\n",
      "15 other europe\n",
      "16 Australia\n",
      "17 China\n",
      "18 India\n",
      "19 Japan\n",
      "20 Korea\n",
      "21 New Zealand\n",
      "22 Pakistan\n",
      "23 Thailand\n",
      "24 other apac\n",
      "25 Brazil\n",
      "26 Canada\n",
      "27 Mexico\n",
      "28 United States\n",
      "29 other americas\n",
      "30 Africa\n",
      "31 Middle East\n",
      "Enter no of country what you want to scrape :4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of Scraped Companies:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 30/39 [08:11<02:08, 14.32s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    scraper = scraping_enfsolar_seller_pages()\n",
    "    driver = scraper.chrome()\n",
    "    country, urls = scraper.open_enfsolar_seller_page_to_scrape_country()\n",
    "    for (i, item) in enumerate(country, start=1):\n",
    "        print(i, item)\n",
    "    Country_no = int(input('Enter no of country what you want to scrape :'))\n",
    "    base_url1 = \"https://www.enfsolar.com\"\n",
    "    base_url = base_url1 + urls[Country_no-1]\n",
    "    driver.get(base_url)\n",
    "    parsed_enf_solar_HTML = bs(driver.page_source, 'lxml')\n",
    "    ylist, company_name, areas = scraper.get_page_results_info(parsed_enf_solar_HTML) \n",
    "    ylist_f = ylist\n",
    "    company_name_f = company_name\n",
    "    areas_f = areas\n",
    "    tbody = bs(driver.page_source, 'lxml').find_all('td',  width='22%')\n",
    "    reference_product_list = [x.get_text().strip() for x in tbody]\n",
    "    reference_product_list.insert(0,'Service Coverage')\n",
    "    reference_product_list.insert(1,'Languages Spoken')\n",
    "    reference_product_list[4] = 'Battery'\n",
    "    info1_data = []\n",
    "    service_coverage_language_spoken_final_list_data = []\n",
    "    product_details_final_list_data =[]\n",
    "    product_title_and_details_final_list_data = []\n",
    "    links_of_company = []\n",
    "    for links in parsed_enf_solar_HTML.find('table', attrs = {'class' : 'enf-list-table'}).find_all('a', href=True):    \n",
    "        links_of_company.append(links.get('href'))  \n",
    "    if  bool(parsed_enf_solar_HTML.find('ul', class_='pagination enf-pagination')) == True :\n",
    "        number_of_pages = parsed_enf_solar_HTML.find('ul', class_='pagination enf-pagination').get_text().strip()\n",
    "        number_of_pages = re.findall(r'\\d+', number_of_pages)\n",
    "        number_of_pages = max(list(map(int, number_of_pages)))\n",
    "        for page in range(2,number_of_pages+1):\n",
    "            base_url_page_number = base_url1 + urls[Country_no-1] + '?page=' + '{}'.format(str(page))\n",
    "            driver.get(base_url_page_number)\n",
    "            parsed_enf_solar_new_page_HTML = bs(driver.page_source, 'lxml')\n",
    "            try:\n",
    "                links_page_new = parsed_enf_solar_new_page_HTML.find('table', attrs = {'class' : 'enf-list-table'}).find_all('a', href=True)\n",
    "                ylist, company_name, areas = scraper.get_page_results_info(parsed_enf_solar_new_page_HTML)         \n",
    "                ylist_f = ylist_f + ylist \n",
    "                company_name_f = company_name_f + company_name\n",
    "                areas_f = areas_f + areas\n",
    "                for new_links in links_page_new:\n",
    "                    links_of_company.append(new_links.get('href'))\n",
    "            except:\n",
    "                pass \n",
    "    bar = tqdm(range(len(links_of_company)))\n",
    "    bar.set_description(\"Number of Scraped Companies\")\n",
    "    for i in bar:\n",
    "        driver.get(links_of_company[i])\n",
    "        driver.implicitly_wait(4)\n",
    "        parsed_company_info_HTML = bs(driver.page_source, 'lxml')\n",
    "        try:\n",
    "            phone_click = driver.find_element(By.XPATH, \"//td[@class='ar:number-direction']//span[@style='cursor: pointer;']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            phone_click = driver.find_element(By.XPATH, \"//div[@itemprop='telephone']//span[@style='cursor: pointer;']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(randint(1,2))\n",
    "        try:\n",
    "            mail_click = driver.find_element(By.XPATH, \"//td[@itemprop='email']//span[@style='cursor: pointer;']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            mail_click = driver.find_element(By.XPATH, \"//div[@itemprop='email']//span[@style='cursor: pointer;']\").click()\n",
    "        except:\n",
    "            pass\n",
    "        if  bool(bs(driver.page_source, 'lxml').find('div', class_='enf-company-profile-info-main-spec position-relative')) == True :\n",
    "            time.sleep(randint(1,2))\n",
    "            script = bs(driver.page_source, 'lxml')\n",
    "            try:\n",
    "                company_profile_info_address = script.find('td', itemprop='address').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_address = 'There is no address information'\n",
    "            try:\n",
    "                company_profile_info_telephone = script.find('td', itemprop='telephone').get_text().strip() \n",
    "            except:\n",
    "                company_profile_info_telephone = 'There is no telephone information'\n",
    "            try:\n",
    "                company_profile_info_email = script.find('td', itemprop='email').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_email = 'There is no email information'\n",
    "            try:\n",
    "                company_profile_info_url = script.find('a', itemprop='url').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_url = 'There is no url information'\n",
    "            try:\n",
    "                company_profile_info_country = script.find('img', class_='w-[22px] ml-px -mt-px mr-[3px] inline-block').get('title')\n",
    "            except:\n",
    "                company_profile_info_country = 'There is no country information'        \n",
    "            business_and_product_names = [x.get_text().strip() for x in script.find('div', id='seller').findAll('div', class_='col-xs-2 enf-section-body-title')]\n",
    "            business_and_product_details = [x.get_text().strip() for x in script.find('div', id='seller').findAll('div', class_='col-xs-10 enf-section-body-content blue')]    \n",
    "            business_and_product_details = scraper.replace(business_and_product_details)\n",
    "        else:\n",
    "            new_style_script = bs(driver.page_source, 'lxml')\n",
    "            try:\n",
    "                company_profile_info_address = new_style_script.find('div', class_='word').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_address = 'There is no address information'\n",
    "            try:\n",
    "                company_profile_info_telephone = new_style_script.find('div', itemprop ='telephone').get_text().strip() \n",
    "            except:\n",
    "                company_profile_info_telephone = 'There is no telephone information'\n",
    "            try:\n",
    "                company_profile_info_email = new_style_script.find('div', itemprop ='email').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_email = 'There is no email information'\n",
    "            try:\n",
    "                company_profile_info_url = new_style_script.find('a', itemprop ='url').get_text().strip()\n",
    "            except:\n",
    "                company_profile_info_url = 'There is no url information'\n",
    "            try:\n",
    "                company_profile_info_country = new_style_script.find('img', class_='w-[22px] ml-px -mt-px mr-[3px] inline-block').get('title')\n",
    "            except:\n",
    "                company_profile_info_country = 'There is no country information'           \n",
    "            business_and_product_names = [x.get_text().strip() for x in bs(driver.page_source, 'lxml').findAll('td', class_='info-title')]\n",
    "            business_and_product_details = [x.get_text().strip() for x in bs(driver.page_source, 'lxml').findAll('td', class_='info-content')]\n",
    "            business_and_product_details = scraper.replace(business_and_product_details)\n",
    "        df4 = pd.DataFrame()\n",
    "        df4['Company Name']  = company_name_f\n",
    "        df4['Areas']  = areas_f\n",
    "        df7 = scraper.dataframes(df4,reference_product_list,business_and_product_names,business_and_product_details) \n",
    "    scraper.save_csv(df7,country,Country_no)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
